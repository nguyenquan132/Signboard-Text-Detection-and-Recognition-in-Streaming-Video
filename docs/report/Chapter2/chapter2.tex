\chapter{CƠ SỞ LÝ THUYẾT VÀ CÁC NGHIÊN CỨU LIÊN QUAN}

\ifpdf
    \graphicspath{{Chapter2/Chapter2Figs/PNG/}{Chapter2/Chapter2Figs/PDF/}{Chapter2/Chapter2Figs/Chapter2Figs/}}
\else
    \graphicspath{{Chapter2/Chapter2Figs/EPS/}{Chapter2/Chapter2Figs/}}
\fi

\markboth{\MakeUppercase{\thechapter. Cơ sở lý thuyết và các nghiên cứu liên quan}}{\thechapter. Cơ sở lý thuyết và các nghiên cứu liên quan}

\section{Tổng quan và ý nghĩa thực tiễn của bài toán phát hiện và nhận dạng văn bản trên biển hiệu trong video đường phố Việt Nam}
Trong môi trường giao thông đô thị tại Việt Nam, biển hiệu chứa đựng lượng lớn thông tin ngữ nghĩa cấp cao, phản ánh trực tiếp danh tính (tên cửa hàng, địa điểm) và loại hình kinh doanh/dịch vụ. Việc tự động trích xuất và hiểu các thông tin này từ luồng video không chỉ giúp giảm bớt việc gán nhãn dữ liệu thủ công mà còn mở ra nhiều ứng dụng thực tiễn hữu ích, có thể kể đến như:
\begin{itemize}
    \item \textbf{Hệ thống dẫn đường thông minh:} Bổ sung thông tin các địa điểm thực tế (tên cửa hàng, địa điểm) từ biển hiệu vào hệ thống dẫn đường, giúp cải thiện độ chính xác của định vị và điều hướng.
    \item \textbf{Phân tích thông tin đô thị:} Tự động thống kê và phân loại các loại hình kinh doanh theo tuyến đường hoặc khu vực, phục vụ quy hoạch và nghiên cứu thị trường.
    \item \textbf{Cập nhật và làm giàu bản đồ số:} Tích hợp thông tin từ biển hiệu để tự động cập nhật cơ sở dữ liệu địa lý (GIS).
\end{itemize}
Trong bối cảnh này, để hiện thực hóa các ứng dụng trên, bài toán đặt ra nhiều thách thức kỹ thuật. Ngoài những khó khăn chung của nhận dạng văn bản trong cảnh (như đa dạng phông chữ, điều kiện ánh sáng), việc xử lý trong bối cảnh video hành trình tại Việt Nam còn phải đối mặt với: chất lượng hình ảnh thay đổi liên tục, góc quay và khoảng cách khác nhau, cùng sự xuất hiện của các biển hiệu với thiết kế đa dạng và ngôn ngữ phức tạp (kết hợp tiếng Việt và tiếng Anh). Những thách thức này nhấn mạnh tầm quan trọng và tính thực tiễn của việc nghiên cứu một giải pháp hiệu quả, phù hợp với đặc thù của bài toán.

\section{Phát hiện đối tượng}
\subsection{Cơ sở và hướng tiếp cận chung}
Phát hiện đối tượng (Object Detection) là một bài toán trong lĩnh vực Thị giác Máy
tính (Computer Vision), đóng vai trò trung tâm trong nhiều ứng dụng thực tiễn như giám
sát an ninh, lái xe tự động, và tương tác người máy. Khác với nhiệm vụ phân loại ảnh
truyền thống vốn chỉ xác định loại đối tượng xuất hiện trong toàn bộ ảnh, phát hiện đối
tượng yêu cầu mô hình không chỉ nhận diện đúng loại đối tượng mà còn xác định chính
xác vị trí của chúng thông qua các hộp giới hạn (bounding boxes). Thách thức của bài
toán này nằm ở việc phải xử lý đồng thời nhiều đối tượng với sự đa dạng lớn về kích
thước, tư thế, góc nhìn, điều kiện ánh sáng và mức độ chồng lấn giữa các đối tượng. Hình ~\ref{fig:chapter2_object_detection} minh họa tổng quan quy trình phát hiện đối tượng.

\begin{figure}[t]
    \centering
    % TODO: cập nhật đường dẫn theo project của bạn
    \includegraphics[width=1\linewidth]{assets/chapter2/object_detection_process.png}
    \caption{Hình ảnh minh họa tổng quan quy trình phát hiện đối tượng. Ảnh đầu vào được xử lý qua mạng nơ-ron để trích xuất đặc trưng và dự đoán vị trí (hộp giới hạn) cùng phân loại các đối tượng (nhãn lớp) xuất hiện trong khung hình.}
    \label{fig:chapter2_object_detection}
\end{figure}

\subsection{Các nghiên cứu liên quan}
Trong những năm gần đây, cùng với sự phát triển mạnh mẽ của học sâu (deep learning), bài toán phát hiện đối tượng đã đạt được những bước tiến vượt bậc cả về độ chính xác lẫn tốc độ, thúc đẩy sự phát triển mạnh mẽ của nhiều ứng dụng thực tế như giám sát thông minh, phân tích video, robot tự hành và xe tự lái. Sự gia tăng về độ phức tạp của dữ liệu ảnh, cùng yêu cầu ngày càng cao về độ chính xác và tốc độ xử lý, đã dẫn đến sự ra đời của nhiều hướng tiếp cận khác nhau cho bài toán này. Dựa trên khảo sát tổng quan của \cite{zou2023object}, các phương pháp phát hiện đối tượng tiên tiến hiện nay có thể được phân loại thành ba hướng tiếp cận chính xét theo kiến trúc và quy trình xử lý: 

\begin{itemize}
    \item \textbf{Các phương pháp hai giai đoạn (Two-Stage):} Các phương pháp hai giai đoạn tiếp cận bài toán phát hiện đối tượng bằng cách tách biệt quá trình đề xuất vùng chứa đối tượng (region proposal) và quá trình phân loại định vị chi tiết. Nhóm này tiêu biểu bởi các mô hình thuộc họ R-CNN, chẳng hạn như \textbf{R-CNN \cite{girshick2014rich}}, \textbf{Fast R-CNN \cite{girshick2015fast}} và \textbf{Faster R-CNN \cite{ren2015faster}}. Trong đó, R-CNN sử dụng các thuật toán đề xuất vùng thủ công kết hợp với CNN để trích xuất đặc trưng, trong khi Fast R-CNN cải thiện hiệu quả bằng cách chia sẻ đặc trưng toàn ảnh. Faster R-CNN tiếp tục nâng cao hiệu suất bằng cách giới thiệu mạng Region Proposal Network (RPN), cho phép học tự động các vùng đề xuất.

    Nhờ khả năng tách biệt rõ ràng giữa phát hiện và phân loại, các phương pháp hai giai đoạn thường đạt độ chính xác cao, đặc biệt trong các kịch bản phức tạp, nhưng đòi hỏi chi phí tính toán lớn và tốc độ xử lý chậm.
    \item \textbf{Các phương pháp một giai đoạn (One-Stage):} Khác với các phương pháp hai giai đoạn, các mô hình một giai đoạn thực hiện trực tiếp việc dự đoán nhãn lớp và hộp giới hạn trong một bước duy nhất, không cần cơ chế đề xuất vùng riêng biệt. Với các đại diện nổi bật như \textbf{YOLO \cite{redmon2016you}}, \textbf{SSD \cite{liu2016ssd}} và \textbf{RetinaNet \cite{lin2017focal}}. YOLO tiếp cận phát hiện đối tượng như một bài toán hồi quy toàn cục, cho phép suy luận nhanh và phù hợp với các ứng dụng thời gian thực. SSD khai thác đặc trưng đa tỷ lệ nhằm cải thiện khả năng phát hiện các đối tượng có kích thước khác nhau. RetinaNet giải quyết vấn đề mất cân bằng giữa các lớp thông qua hàm mất mát Focal Loss, giúp nâng cao độ chính xác cho các đối tượng khó phát hiện.
    
    Nhìn chung, các phương pháp một giai đoạn (One-Stage) đạt được sự cân bằng tốt giữa tốc độ và độ chính xác, nhưng đôi khi kém ổn định hơn trong các bối cảnh có mật độ đối tượng cao hoặc chồng lấn mạnh.
    \item \textbf{Các phương pháp dựa trên Transformer:} Gần đây, các phương pháp dựa trên Transformer đã tạo ra một bước chuyển quan trọng trong phát hiện đối tượng bằng cách xây dựng kiến trúc end-to-end, loại bỏ các thành phần được thiết kế thủ công như anchor boxes và thuật toán Non-Maximum Suppression (NMS). Điển hình cho hướng đi này là mô hình \textbf{DETR \cite{carion2020end}} trong đó bài toán phát hiện đối tượng được mô hình hóa như một bài toán gán tập (set prediction) thông qua cơ chế self-attention. Các biến thể sau đó của DETR tập trung vào cải thiện tốc độ hội tụ và hiệu suất suy luận, mở ra hướng nghiên cứu mới cho các mô hình phát hiện đối tượng. Bên cạnh đó, khả năng mô hình hóa quan hệ toàn cục và thiết kế kiến trúc end-to-end của Transformer cũng đã chứng minh hiệu quả trong nhiều bài toán thị giác máy tính liên quan, bao gồm cả phân đoạn ảnh.
\end{itemize}
% \subsection{Phương pháp tiếp cận}

% Trong bối cảnh của khóa luận này, bài toán phát hiện biển hiệu đòi hỏi sự cân bằng giữa tốc độ xử lý, độ chính xác và khả năng xử lý các đối tượng có hướng (oriented objects). Vì vậy, khóa luận tập trung lựa chọn và đánh giá một số phương pháp tiên tiến hiện nay, tiêu biểu cho các hướng tiếp cận khác nhau, dựa trên mức độ phù hợp với các yêu cầu của bài toán.

% \begin{itemize}
%     \item \textbf{YOLO (You Only Look Once) \cite{redmon2016you}:} YOLO là đại diện tiêu biểu cho hướng tiếp cận một giai đoạn (one-stage). Kiến trúc của YOLO dựa trên việc chia ảnh đầu vào thành một lưới (grid), mỗi ô lưới chịu trách nhiệm dự đoán đồng thời các bounding box và xác suất lớp. Cách tiếp cận trực tiếp này mang lại tốc độ suy luận rất cao, phù hợp cho các ứng dụng thời gian thực. Đặc biệt, biến thể YOLO-OBB (Oriented Bounding Box) mở rộng khả năng phát hiện vật thể xoay, là một lựa chọn rất phù hợp cho bài toán phát hiện biển hiệu.
%     \item \textbf{DETR (DEtection TRansformer) \cite{carion2020end}:} DETR là mô hình phát hiện đối tượng đầu tiên hoàn toàn dựa trên kiến trúc Transformer. Mô hình sử dụng một tập hợp cố định các "truy vấn" (object queries) để tương tác với đặc trưng hình ảnh và dự đoán trực tiếp một tập các bounding box, nhờ đó loại bỏ hoàn toàn nhu cầu về các thành phần thủ công như anchor boxes và NMS.
%     \item \textbf{RTDETRv2 \cite{zhao2024detrs}:} RT-DETRv2 là phiên bản cải tiến từ RT-DETR, được đề xuất với mục tiêu tối ưu hóa hiệu suất thời gian thực (real-time performance) trong khi vẫn duy trì độ chính xác cao. Mô hình này giữ nguyên ưu điểm end-to-end của DETR, loại bỏ sự phụ thuộc vào NMS, và được tối ưu hóa thông qua các cơ chế như hybrid encoder cùng cơ chế lựa chọn truy vấn (query selection) nhằm cân bằng giữa độ chính xác và hiệu quả tính toán.

% \end{itemize}

% Bên cạnh các phương pháp phát hiện trực tiếp dựa trên bounding box, để mở rộng góc nhìn đánh giá, khóa luận xem xét một hướng tiếp cận gián tiếp thông qua bài toán phân đoạn ngữ nghĩa (semantic segmentation). Theo hướng tiếp cận này, đối tượng trước hết được phân đoạn ở mức điểm ảnh, từ đó suy ra vùng bao hình học của đối tượng, phục vụ cho bài toán phát hiện. Trong bối cảnh đó, theo tổng quan của \cite{thisanke2023semantic}, các kiến trúc dựa trên Transformer đã trở thành một hướng tiếp cận được quan tâm rộng rãi và ngày càng quan trọng trong bài toán phân đoạn ảnh, đặc biệt là phân đoạn ngữ nghĩa. Nhờ khả năng mô hình hóa ngữ cảnh toàn cục thông qua cơ chế self-attention, các mô hình này cho thấy hiệu quả nổi bật trong việc xử lý các kịch bản có cấu trúc phức tạp và sự đa dạng lớn về hình dạng đối tượng.

% \begin{itemize}
%     \item \textbf{SegFormer \cite{xie2021segformer}:} SegFormer là một kiến trúc phân đoạn ngữ nghĩa hiệu quả dựa trên Transformer, kết hợp giữa encoder Transformer phân cấp và decoder MLP nhẹ. Thiết kế này cho phép mô hình khai thác ngữ cảnh toàn cục thông qua self-attention, đồng thời duy trì hiệu quả tính toán cao nhờ cấu trúc decoder đơn giản. Nhờ đó, SegFormer đạt được sự cân bằng tốt giữa độ chính xác và tốc độ suy luận, phù hợp với các ứng dụng phân đoạn ngữ nghĩa trong bối cảnh thực tế.
%     \item \textbf{Mask2Former \cite{cheng2022masked}:} Mask2Former là một kiến trúc phân đoạn dựa trên Transformer theo hướng tiếp cận thống nhất (unified framework), có khả năng xử lý nhiều bài toán phân đoạn khác nhau như phân đoạn ngữ nghĩa (semantic segmentation), phân đoạn theo thể hiện (instance segmentation) và phân đoạn toàn cảnh (panoptic segmentation). Mask2Former áp dụng cơ chế masked attention, cho phép mô hình tập trung vào các vùng đối tượng tiềm năng thông qua các mask dự đoán, từ đó cải thiện khả năng biểu diễn hình dạng và biên đối tượng. Cách tiếp cận này giúp Mask2Former thể hiện hiệu quả trên các trường hợp có đối tượng chồng lấn hoặc hình dạng phức tạp.
% \end{itemize}

\section{Phát hiện và nhận dạng văn bản ngoại cảnh (Scene Text Detection and Recogntion - STDR)}
\subsection{Phát hiện văn bản ngoại cảnh (Scene Text Detection - STD)}
\subsubsection{Cơ sở và hướng tiếp cận chung}
Phát hiện văn bản trong ảnh ngoại cảnh (Scene Text Detection) hướng tới mục tiêu xác định và khoanh vùng các khu vực chứa văn bản. Khác với các tác vụ phát hiện đối tượng truyền thống, phát hiện văn bản trong ảnh ngoại cảnh phải đối mặt với nhiều thách thức do sự đa dạng về hình dạng, kích thước, hướng và bố cục của văn bản, cũng như các trường hợp văn bản bị nghiêng, cong, chồng chéo hoặc mờ.  Do đó, bài toán này đòi hỏi kết hợp các kỹ thuật phát hiện đối tượng với các phương pháp chuyên biệt cho văn bản nhằm xác định chính xác và hiệu quả các vùng chứa văn bản.

\subsubsection{Các nghiên cứu liên quan}

Dựa trên các nghiên cứu khảo sát và tổng quan gần đây \cite{kang2022overview, pal2024comprehensive, naiemi2022scene, han2024spotlight} về phát hiện văn bản trong ảnh ngoại cảnh, các phương pháp tiên tiến hiện nay có thể được phân thành ba nhóm chính: (i) dựa trên hồi quy (regression-based), (ii) dựa trên phân đoạn (segmentation-based) và (iii) dựa trên thành phần liên thông (connected component-based).

\begin{itemize}
    \item \textbf{Các phương pháp dựa trên hồi quy (Regression-based):} Hướng tiếp cận này giải quyết bài toán phát hiện văn bản tương tự như phát hiện đối tượng, bằng cách trực tiếp dự đoán tọa độ các vùng văn bản dưới dạng hộp chữ nhật hoặc đa giác. Nhờ kiến trúc tối ưu cho việc dự đoán tọa độ, các phương pháp này thường có tốc độ suy luận nhanh, phù hợp với các ứng dụng thời gian thực. Liao và cộng sự đã đề xuất \textbf{TextBoxes \cite{liao2017textboxes}}, với điều chỉnh hình dạng convolutional kernel và anchor của SSD để phù hợp với tỷ lệ co giãn đa dạng của văn bản cảnh, cải thiện khả năng phát hiện văn bản đa hướng. \textbf{EAST \cite{zhou2017east}} đề xuất một pipeline hiệu quả, dự đoán trực tiếp khung bao xoay (rotated box) hoặc tứ giác (quadrangle) từ đặc trưng hình ảnh, loại bỏ sự phụ thuộc vào các bước đề xuất vùng (region proposal) phức tạp. Để xử lý văn bản hình dạng bất kỳ, Liu và cộng sự đề xuất \textbf{ABCNet \cite{liu2020abcnet}}, sử dụng đường cong Bezier (Bezier curve) làm biểu diễn tham số hóa linh hoạt cho đường biên văn bản, cho phép mô hình hóa chính xác các văn bản cong. \textbf{FCE-Net \cite{zhu2021fourier}} đề xuất một cách biểu diễn khác thông qua phép nhúng chuỗi Fourier (Fourier contour embedding), giúp biểu diễn hiệu quả và tái tạo các đường biên văn bản phi chuẩn từ đầu ra hồi quy.
    
    Tuy nhiên, một hạn chế chung của hướng tiếp cận này là sự phụ thuộc vào các bước hậu xử lý (post-processing) phức tạp để phục hồi thể hiện văn bản từ đầu ra hồi quy.

    \item \textbf{Các phương pháp dựa trên thành phần liên thông (Connected Component-based):} Các hương pháp này tập trung vào việc phát hiện và nhóm các thành phần ảnh có đặc trưng tương đồng (như màu sắc, kết cấu hoặc cường độ) để hình thành các vùng văn bản hoàn chỉnh. Long và cộng sự đề xuất \textbf{TextSnake \cite{long2018textsnake}}, mô hình hóa văn bản cong bằng một chuỗi các hình tròn linh hoạt (các "vảy rắn") dọc theo trục trung tâm. Baek và cộng sự \textbf{CRAFT \cite{baek2019character}} dự đoán bản đồ "affinity" giữa các ký tự thông qua học chuyển giao từ dữ liệu tổng hợp, cung cấp hướng dẫn rõ ràng cho việc nhóm. \textbf{DRRG \cite{zhang2020deep}} được đề xuất bởi Zhang và cộng sự, sử dụng Mạng Tích chập Đồ thị (Graph Convolutional Network - GCN) để mô hình hóa mối quan hệ cấu trúc giữa các thành phần văn bản và thực hiện việc nhóm một cách thông minh.
    
    Mặc dù có thể biểu diễn chính xác các văn bản cong, hiệu quả cuối cùng của hướng tiếp cận này phụ thuộc nhiều vào các thuật toán nhóm (grouping) phức tạp, điều này có thể ảnh hưởng đến tốc độ xử lý và độ ổn định tổng thể.
    \item \textbf{Các phương pháp dựa trên phân đoạn (Segmentation-based):} Nhóm phương pháp này xem phát hiện văn bản như một bài toán phân đoạn mức pixel, phân loại mỗi pixel là văn bản hoặc nền, sau đó suy ra các vùng văn bản từ kết quả phân đoạn. \textbf{PANet \cite{liu2018path}} được đề xuất để tổng hợp đặc trưng đa tỷ lệ và nhóm chính xác các pixel văn bản. Liao và cộng sự đề xuất \textbf{DBNet++ \cite{liao2022real}}, tích hợp differentiable binarization và Adaptive Scale Fusion nhằm giảm thiểu bước hậu xử lý và cải thiện độ chính xác. \textbf{TextPMs \cite{zhang2022arbitrary}} sử dụng nhóm bản đồ xác suất và mô hình học lặp để phục hồi văn bản cong một cách chính xác. \textbf{FAST \cite{chen2021fast}} và \textbf{KPN \cite{zhang2022kernel}} tập trung vào việc xử lý hiệu quả các văn bản phi chuẩn và đa tỷ lệ.
    
    Nhìn chung, phương pháp này đặc biệt hiệu quả trong việc xử lý các văn bản có hình dạng cong, hoặc nghiêng. Tuy nhiên, nó thường đòi hỏi chi phí tính toán cao hơn so với các phương pháp dựa trên hồi quy trực tiếp.

\end{itemize}

% \subsubsection{Phương pháp tiếp cận}
% Trong bối cảnh đầy thách thức của phát hiện văn bản trên biển hiệu thực tế, khóa luận lựa chọn một số phương pháp tiên tiến hiện nay nhằm đánh giá khả năng xử lý các văn bản có hình dạng đa dạng. Những phương pháp này có khả năng duy trì độ chính xác đồng thời giảm thiểu các bước hậu xử lý phức tạp. Việc đánh giá các mô hình không chỉ giúp so sánh hiệu quả giữa các giải pháp hiện nay, mà còn cung cấp cái nhìn tổng quan về những hướng tiếp cận khả thi cho bài toán.

% \begin{itemize}
%     \item \textbf{PANet \cite{liu2018path}:} PANet là một kiến trúc phân đoạn hiệu quả với hai thành phần chính gồm Feature Pyramid Enhancement Module (FPEM), chịu trách nhiệm tạo bản đồ đặc trưng đa tỷ lệ, và Feature Fusion Module (FFM), thực hiện tổng hợp các đặc trưng này. Bằng cách áp dụng phương pháp pixel aggregation trên bản đồ đặc trưng cuối cùng, PANet có thể nhóm chính xác các pixel văn bản vào các thể hiện tương ứng, đạt hiệu quả cao nhờ quy trình phân đoạn có chi phí tính toán thấp.
%     \item \textbf{DBNet \cite{liao2022real}:} DBNet++ là phiên bản cải tiến của DBNet, tích hợp cơ chế differentiable binarization (DB) trực tiếp vào mạng phân đoạn để tạo mask văn bản chính xác và ổn định, giảm đáng kể các bước hậu xử lý. Đồng thời, Adaptive Scale Fusion (ASF) được áp dụng để hợp nhất các đặc trưng đa tỷ lệ, cải thiện khả năng xử lý các văn bản có kích thước khác nhau.
%     \item \textbf{TextPMs \cite{zhang2022arbitrary}:}
%     \item \textbf{FAST \cite{chen2021fast}:}
%     \item \textbf{KPN \cite{zhang2022kernel}:}
% \end{itemize}

\subsection{Nhận dạng văn bản (Scene Text Recognition - STR)}
\subsubsection{Cơ sở và hướng tiếp cận chung}
Nhận dạng văn bản trong ảnh ngoại cảnh (Scene Text Recognition) là một bài toán phức tạp trong lĩnh vực Thị giác Máy tính, liên quan đến việc đọc và xác định nội dung của văn bản xuất hiện trong các cảnh ảnh thực tế. Khác với các bài toán nhận dạng văn bản truyền thống trên tài liệu hoặc bảng biểu, văn bản ngoại cảnh thường xuất hiện trong môi trường không đồng nhất, chịu biến dạng, thay đổi lớn về ánh sáng, góc chụp, nền, phông chữ và hình thức trình bày. Mục tiêu là nhận diện chính xác nội dung của các ký tự và từ ngữ trong các vùng văn bản đã được khoanh vùng (cropped text instances), chuyển đổi từng hình ảnh văn bản riêng lẻ thành chuỗi ký tự tương ứng. Bài toán không chỉ đòi hỏi nhận diện ký tự riêng lẻ mà còn cần hiểu ngữ cảnh tổng thể của văn bản trong ảnh, bao gồm mối quan hệ giữa các ký tự, từ ngữ và bố cục, đặc biệt trong các điều kiện biến dạng, cong, nghiêng hoặc không chuẩn.  

% \begin{figure}[t]
%     \centering
%     % TODO: cập nhật đường dẫn theo project của bạn
%     \includegraphics[width=1\linewidth]{assets/chapter2/scene_text_recognition.png}
%     \caption{Hình ảnh minh họa quá trình nhận dạng văn bản ngoại cảnh (STR)}
%     \label{fig:chapter2_end_to_end_STR}
% \end{figure}

\subsubsection{Các nghiên cứu liên quan}
Scene Text Recognition (STR) là một lĩnh vực nghiên cứu thu hút sự quan tâm mạnh
mẽ trong cộng đồng thị giác máy tính. Trong các nghiên cứu khảo sát và tổng quan gần đây \cite{kang2022overview, pal2024comprehensive, naiemi2022scene, do2024signboardtext}, bài toán nhận dạng văn bản trong ảnh ngoại cảnh có thể được chia thành 2 loại chính dựa trên nguyên lý làm việc: (i) các phương pháp dựa trên phân đoạn ký tự (segmentation-based) và (ii) các phương pháp không dựa trên phân đoạn (segmentation-free).

\begin{itemize}
    \item \textbf{Các phương pháp dựa trên phân đoạn ký tự (Segmentation-based):} Nhóm phương pháp này tiếp cận bài toán STR bằng cách dự đoán nhãn mức pixel cho từng ký tự hoặc thành phần ký tự, sau đó thực hiện nhận dạng dựa trên kết quả phân đoạn. \textbf{MaskTextSpotter \cite{lyu2018mask}} được đề xuất bởi Lyu và cộng sự, sử dụng mạng phân đoạn ký tự kết hợp cơ chế spatial attention để nhận dạng văn bản hình dạng bất kỳ, khắc phục hạn chế do thiếu dữ liệu chú thích cấp ký tự. Để cải thiện độ chính xác trong bối cảnh phức tạp, Ye và cộng sự đề xuất \textbf{TextFuseNet \cite{ye2020textfusenet}}, một kiến trúc tích hợp thông tin đa cấp (character-, word-, và global-level), giúp phân đoạn ký tự mạnh mẽ hơn.
    
    Tuy đạt hiệu quả cao trong việc biểu diễn văn bản phi chuẩn, các phương pháp trong nhóm này thường phụ thuộc vào chất lượng của bước phân đoạn ký tự, đồng thời đòi hỏi quy trình hậu xử lý phức tạp, dẫn đến chi phí tính toán cao và khó mở rộng trong các kịch bản thực tế
    \item \textbf{Các phương pháp không dựa trên phân đoạn (Segmentation-free):} Nhóm phương pháp này tập trung vào việc trực tiếp ánh xạ toàn bộ vùng ảnh văn bản (word hoặc text line) thành chuỗi ký tự đầu ra, thông qua một kiến trúc encoder\allowbreak-\allowbreak decoder. Các phương pháp truyền thống trong nhóm này thường sử dụng mạng CNN để trích xuất đặc trưng thị giác, kết hợp với mô hình chuỗi như BiLSTM để nắm bắt quan hệ ngữ cảnh, và sử dụng CTC hoặc attention làm cơ chế dự đoán. Tiêu biểu là \textbf{CRNN \cite{shi2016end}}, trong đó Shi và cộng sự kết hợp CNN, RNN và CTC loss để thực hiện nhận dạng chuỗi ký tự một cách hiệu quả. 
    
    Gần đây, các phương pháp tiên tiến dựa trên Transformer đã đạt được nhiều kết quả vượt trội. \textbf{ViTSTR \cite{atienza2021vision}} được đề xuất nhằm áp dụng Vision Transformer trực tiếp cho bài toán STR, khai thác cơ chế self-attention để mô hình hóa quan hệ toàn cục trong chuỗi đặc trưng. \textbf{PARSeq \cite{bautista2022scene}} tiếp tục mở rộng hướng tiếp cận này bằng cách kết hợp mô hình Transformer tự hồi quy với ngữ cảnh hai chiều, giúp cải thiện độ chính xác trong nhận dạng chuỗi dài và phức tạp. Bên cạnh đó, một số nghiên cứu gần đây như \textbf{CDistNet \cite{zheng2024cdistnet}}, \textbf{SMTR \cite{du2025out}} và \textbf{SVTRv2 \cite{u2025svtrv2}} tập trung vào việc thiết kế kiến trúc hiệu quả hơn cho STR, thông qua cải tiến cơ chế trích xuất đặc trưng, mô hình hóa chuỗi hoặc tối ưu hóa cấu trúc Transformer, nhằm cân bằng giữa độ chính xác và chi phí tính toán.

    Nhìn chung, hướng tiếp cận này có kiến trúc tương đối gọn nhẹ và khả năng mở rộng tốt nhờ không phụ thuộc vào chú thích ký tự chi tiết. Tuy nhiên, việc không sử dụng phân đoạn tường minh khiến các phương pháp này gặp hạn chế khi xử lý văn bản có hình dạng phức tạp, cong hoặc biến dạng mạnh.
\end{itemize}
\subsection{Nhận dạng văn bản ngoại cảnh đầu–cuối (End-to-End Scene Text Recognition)}
\subsubsection{Cơ sở và hướng tiếp cận chung}
Nhận dạng văn bản ngoại cảnh đầu–cuối (End-to-End Scene Text Recognition hướng tới mục tiêu giải quyết đồng thời cả hai bài toán phát hiện văn bản (text detection) và nhận dạng văn bản (text recognition) trong một pipeline thống nhất, thay vì xử lý chúng như hai tác vụ tách biệt. Khác với các hệ thống truyền thống theo pipeline tuần tự, trong đó kết quả phát hiện văn bản được sử dụng làm đầu vào cho bước nhận dạng, các phương pháp end-to-end tìm cách tối ưu hóa toàn bộ quá trình từ ảnh đầu vào đến chuỗi ký tự đầu ra một cách thống nhất. Quy trình tổng quát của hướng tiếp cận này được minh họa trong Hình~\ref{fig:chapter2_end_to_end_STR}

Cách tiếp cận này cho phép mô hình học được mối quan hệ chặt chẽ giữa vị trí, hình dạng và nội dung của văn bản trong ảnh, từ đó giảm thiểu sự phụ thuộc vào các bước trung gian và hạn chế sai lệch lan truyền giữa các giai đoạn. Do đó, End-to-End Scene Text Recognition được xem là hướng tiếp cận hiệu quả cho các ứng dụng thực tế đòi hỏi độ chính xác cao và quy trình xử lý gọn nhẹ.

\begin{figure}[t]
    \centering
    % TODO: cập nhật đường dẫn theo project của bạn
    \includegraphics[width=1\linewidth]{assets/chapter2/end_to_end_text_spotting.png}
    \caption{Hình ảnh minh họa quá trình nhận dạng văn bản ngoại cảnh đầu-cuối (End-to-End Scene Text Recognition)}
    \label{fig:chapter2_end_to_end_STR}
\end{figure}


\subsubsection{Các nghiên cứu liên quan}
Trong bối cảnh ngày càng nhiều ứng dụng thực tế yêu cầu trích xuất thông tin văn bản trực tiếp từ ảnh, chẳng hạn như dịch tự động, phân tích nội dung hình ảnh hay hỗ trợ người dùng trong môi trường thông minh, việc xử lý phát hiện và nhận dạng văn bản trong một pipeline thống nhất ngày càng trở nên cần thiết. Chính vì vậy, nhiều nghiên cứu gần đây tập trung vào bài toán nhận dạng văn bản ngoại cảnh đầu-cuối (End-to-End Scene Text Recognition), nhằm đồng thời giải quyết hai nhiệm vụ phát hiện và nhận dạng văn bản trong cùng một pipeline.

Theo các nghiên cứu khảo sát và tổng quan gần đây \cite{kang2022overview, pal2024comprehensive, naiemi2022scene, do2024signboardtext}, bài toán nhận dạng văn bản ngoại cảnh có thể được chia thành hai hướng tiếp cận chính: (i) các phương pháp hai giai đoạn (two-stage scene text spotters) và (ii) các phương pháp một giai đoạn (one-stage scene text spotters).

\begin{itemize}
    \item \textbf{Các phương pháp hai giai đoạn (Two-Stage Scene Text Spotters):} Các phương pháp thuộc nhóm này tiếp cận bài toán text spotting bằng cách kết hợp một mô-đun phát hiện văn bản và một mô-đun nhận dạng văn bản riêng biệt trong một pipeline xử lý tuần tự. Tiêu biểu cho hướng tiếp cận này, \textbf{TextBoxes \cite{liao2017textboxes}} sử dụng bộ phát hiện dựa trên SSD và bộ nhận dạng CRNN, đặt nền móng cho kiến trúc hai giai đoạn trong bài toán text spotting. Tuy nhiên, việc tối ưu tách biệt hai mô-đun có thể gây lan truyền lỗi do thiếu sự phối hợp giữa phát hiện và nhận dạng, từ đó làm suy giảm độ chính xác tổng thể. Gần đây, \textbf{MaskTextSpotter \cite{lyu2018mask}} sử dụng mô-đun Region-of-Interest (RoI) để trích xuất các vùng ứng viên và đưa vào nhánh Fast R-CNN nhằm sinh bản đồ phân đoạn ngữ nghĩa, cho phép xử lý hiệu quả văn bản có hình dạng bất kỳ. Một hướng tiếp cận khác được thể hiện trong \textbf{ABCNet \cite{liu2020abcnet}} với việc sử dụng BezierAlign, một phép biến đổi có tham số học được giúp chuyển đổi chính xác các vùng văn bản hình dạng bất kỳ (đặc biệt là văn bản cong) thành các đặc trưng đầu vào chuẩn cho bộ nhận dạng.
    
    Mặc dù hướng tiếp cận hai giai đoạn (Two-Stage) mang lại hiệu quả cao nhờ khả năng kết hợp các mô-đun phát hiện và nhận dạng mạnh mẽ trong cùng một hệ thống, cấu trúc xử lý tuần tự và sự phụ thuộc vào các bước trung gian như như đề xuất vùng (Region of Interest - RoI) khiến các phương pháp này gặp thách thức về hiệu suất và khả năng mở rộng, đặc biệt trong các ứng dụng yêu cầu xử lý nhanh và gọn.
    \item \textbf{Các phương pháp một giai đoạn (One-Stage Scene Text Spotters):} Nhằm giảm thiểu sự phụ thuộc vào các bước trung gian như đề xuất vùng (Region of Interest - RoI) và đơn giản hóa pipeline xử lý, các phương pháp một giai đoạn tích hợp trực tiếp phát hiện và nhận dạng văn bản vào một mạng duy nhất, cho phép dự đoán văn bản theo cách đầu-cuối (end-to-end). \textbf{PGNet \cite{wang2021pgnet}} dự đoán văn bản một cách trực tiếp thông qua việc học chuỗi các điểm trung tâm. Trong khi đó, \textbf{DeepSolo \cite{ye2023deepsolo}}, lấy cảm hứng từ ABCNet, đề xuất cơ chế biểu diễn đường cong trung tâm Bezier đơn giản hơn kết hợp với công thức truy vấn mới, cho phép phân loại ký tự chỉ thông qua phép chiếu tuyến tính từ các đặc trưng truy vấn.
    
    Bên cạnh đó, một số nghiên cứu gần đây như \textbf{TESTR \cite{zhang2022text}}, \textbf{UNITS \cite{kil2023towards}} và \textbf{DNTextSpotter}\allowbreak~\cite{qiao2024dntextspotter} tập trung vào việc thiết kế kiến trúc thống nhất cho bài toán text spotting, thông qua khai thác Transformer, cơ chế truy vấn hoặc biểu diễn đặc trưng linh hoạt, nhằm cải thiện khả năng học đầu–cuối và giảm sự phụ thuộc vào các bước trung gian.

    Như vậy, hướng tiếp cận một giai đoạn (One-Stage) giúp đơn giản hóa kiến trúc và giảm độ trễ suy luận nhờ loại bỏ các bước xử lý trung gian. Song, việc học đồng thời hai nhiệm vụ trong một kiến trúc duy nhất khiến mô hình dễ gặp khó khăn trong việc cân bằng giữa độ chính xác phát hiện và khả năng nhận dạng, đặc biệt trong các điều kiện dữ liệu phức tạp.
\end{itemize}
% \section{Giới thiệu}

% \subsection{Tổng quan và ý nghĩa thực tiễn của bài toán phát hiện và nhận dạng văn bản trên biển hiệu trong video đường phố Việt Nam}
% Trong môi trường giao thông đô thị, biển hiệu và bảng quảng cáo (signboards) là nguồn thông tin quan trọng phản ánh danh tính địa điểm (tên cửa hàng), cũng như loại sản phẩm/dịch vụ mà địa điểm đó cung cấp.
% Với dữ liệu video quay từ camera hành trình, hệ thống ``đọc văn bản trong cảnh'' (scene text reading) có thể hỗ trợ nhiều ứng dụng thực tế như:
% (i) lập bản đồ/định vị theo ngữ nghĩa (semantic mapping), (ii) tìm kiếm địa điểm theo từ khóa (place search),
% (iii) thống kê loại hình kinh doanh theo khu vực, và (iv) hỗ trợ nhận thức tình huống trong các hệ thống giao thông thông minh.

% Bài toán trong khóa luận tập trung vào xây dựng một pipeline tích hợp gồm:
% \begin{itemize}
%     \item \textbf{Phát hiện (Text Detection):} xác định và khoanh vùng các vùng chứa văn bản trong từng khung hình.
%     \item \textbf{Nhận dạng (Text Recognition):} chuyển đổi ảnh vùng chữ thành chuỗi ký tự.
% \end{itemize}

% \subsection{Thách thức về tính đa dạng và phức tạp của văn bản trong môi trường tự nhiên}
% Khác với tài liệu quét (scanned documents), văn bản trong cảnh đường phố thường xuất hiện trong điều kiện chụp không kiểm soát.
% Các thách thức nổi bật bao gồm:
% \begin{itemize}
%     \item \textbf{Nền phức tạp (cluttered background):} nhiều vật thể/hoa văn gây nhiễu.
%     \item \textbf{Văn bản biến dạng:} chữ cong/nghiêng/méo do phối cảnh, bề mặt biển hiệu hoặc góc nhìn.
%     \item \textbf{Đa dạng phông chữ và kích thước:} font stylized, độ dày nét khác nhau, chữ rất nhỏ hoặc rất lớn.
%     \item \textbf{Đa ngôn ngữ và dấu:} tiếng Việt có dấu, có thể xen kẽ tiếng Anh/Trung/Hàn; dấu câu đa dạng.
%     \item \textbf{Motion blur và out-of-focus:} do xe di chuyển, rung camera, tốc độ cao.
%     \item \textbf{Độ phân giải thấp (low resolution):} chữ nhỏ, ở xa camera, nén video làm mất chi tiết.
% \end{itemize}

% \subsubsection{Bài toán Scene Text Detection and Recognition}
% Tổng quát, bài toán Scene Text Reading có thể được tiếp cận theo ba hướng chính:
% \begin{enumerate}
%     \item \textbf{Text Detection:} chỉ dự đoán vị trí vùng chữ (bounding box / polygon / mask).
%     \item \textbf{Text Recognition:} nhận dạng ký tự/chuỗi ký tự từ các vùng chữ đã được cắt sẵn.
%     \item \textbf{End-to-End Text Spotting/Recognition:} kết hợp phát hiện và nhận dạng trong một pipeline thống nhất.
% \end{enumerate}
% Trong khóa luận, trọng tâm là xây dựng pipeline tích hợp cho dữ liệu street-view/video, ưu tiên nhóm đối tượng biển hiệu/bảng quảng cáo cửa hàng.

% \subsubsection{Phân loại hướng tiếp cận cho bài toán Text Detection and Recognition}
% Các phương pháp học sâu cho bài toán đọc văn bản trong ảnh ngoại cảnh (scene text reading) thường được phân nhóm theo phạm vi xử lý:
% (i) chỉ phát hiện vùng chữ, (ii) chỉ nhận dạng chữ trên vùng cắt sẵn, hoặc (iii) pipeline end-to-end kết hợp cả hai.

% \begin{figure}[t]
%     \centering
%     \includegraphics[width=0.98\linewidth]{text_detection_recognition_taxonomy.png}
%     \caption{Phân nhóm các phương pháp cho bài toán Text Detection and Recognition trong ảnh ngoại cảnh.}
%     \label{fig:text_detection_recognition_taxonomy}
% \end{figure}

% \paragraph{Text Detection.}
% \begin{itemize}
%     \item \textbf{Regression-based:} hồi quy trực tiếp hộp/tứ giác bao quanh vùng chữ.
%     \item \textbf{Connected component-based:} phát hiện thành phần ký tự (hoặc stroke) rồi liên kết thành dòng/từ.
%     \item \textbf{Segmentation-based:} dự đoán bản đồ pixel thuộc text, sau đó tách instance bằng hậu xử lý.
% \end{itemize}

% \paragraph{Text Recognition.}
% \begin{itemize}
%     \item \textbf{Segmentation-based:} tách ký tự (hoặc vùng con) rồi nhận dạng.
%     \item \textbf{Segmentation-free:} nhận dạng trực tiếp chuỗi (CTC/attention/transformer) không cần tách ký tự.
% \end{itemize}

% \paragraph{End-to-End Text Recognition.}
% \begin{itemize}
%     \item \textbf{One-stage:} phát hiện và nhận dạng trong một mô hình thống nhất.
%     \item \textbf{Two-stage:} phát hiện trước, sau đó cắt/chuẩn hóa và nhận dạng ở mô-đun thứ hai.
% \end{itemize}


% \section{Cơ sở lý thuyết}

% \subsection{Biểu diễn dữ liệu video và trích xuất khung hình}
% Video được xem như chuỗi khung hình (frame) theo thời gian.
% Cho video $V$, ta trích xuất tập khung hình $\{I_t\}_{t=1}^{T}$ với tốc độ lấy mẫu phù hợp (ví dụ: lấy mọi frame hoặc lấy theo bước nhảy để tối ưu tính toán).
% Vì văn bản có thể xuất hiện trong nhiều frame liên tiếp, dữ liệu video mang \textit{tính dư thừa theo thời gian} (temporal redundancy) có thể khai thác để tăng độ ổn định.

% \subsection{Quy trình xử lý tổng thể (Integrated Pipeline)}
% Pipeline đề xuất ở mức khái niệm gồm các bước:
% \begin{enumerate}
%     \item \textbf{Tiền xử lý (Pre-processing):}
%     \begin{itemize}
%         \item Giảm nhiễu, cân bằng sáng, tăng tương phản cục bộ khi cần thiết.
%         \item Giảm mờ do chuyển động (deblurring) hoặc tăng độ phân giải (super-resolution) cho vùng chữ nhỏ (tùy tài nguyên).
%         \item Ổn định video (video stabilization) trong trường hợp rung mạnh.
%     \end{itemize}

%     \item \textbf{Phát hiện vùng văn bản (Text Detection):}
%     Dự đoán vị trí vùng chữ theo dạng hộp (box), tứ giác (quadrilateral), đa giác (polygon) hoặc mặt nạ (segmentation mask).
%     Đầu ra gồm tập vùng $\mathcal{B}_t = \{b_{t}^{(i)}\}$ tại frame $I_t$.

%     \item \textbf{Chuẩn hóa hình học và cắt vùng chữ (Crop \& Rectify):}
%     Với vùng chữ nghiêng/cong, cần biến đổi phối cảnh hoặc chuẩn hóa hình học để đưa về ảnh chữ ``thẳng'' (rectified) trước khi nhận dạng.
%     Gọi ảnh vùng chữ sau chuẩn hóa là $\hat{I}_{t}^{(i)}$.

%     \item \textbf{Nhận dạng văn bản (Text Recognition):}
%     Mô hình nhận dạng thực hiện ánh xạ $\hat{I}_{t}^{(i)} \rightarrow \mathbf{s}_{t}^{(i)}$,
%     trong đó $\mathbf{s}_{t}^{(i)}$ là chuỗi ký tự dự đoán.

%     \item \textbf{Hậu xử lý (Post-processing):}
%     \begin{itemize}
%         \item Chuẩn hóa Unicode tiếng Việt, sửa lỗi dấu/telex nếu cần.
%         \item Loại bỏ ký tự nhiễu, lọc theo độ tin cậy (confidence).
%         \item Gộp các kết quả theo thời gian (temporal fusion) nếu cùng một biển hiệu xuất hiện ở nhiều frame.
%     \end{itemize}

%     \item \textbf{Suy luận loại dịch vụ/sản phẩm (Semantic Inference):}
%     Từ chuỗi ký tự $\mathbf{s}$, hệ thống gán nhãn ngành hàng/dịch vụ bằng:
%     (i) luật từ khóa (keyword rules), (ii) phân lớp văn bản (text classification), hoặc (iii) kết hợp NER + taxonomy.
% \end{enumerate}

% \subsection{Cơ sở lý thuyết Text Detection}
% Text Detection trong cảnh có thể chia thành các hướng chính:
% \begin{itemize}
%     \item \textbf{Regression-based:} dự đoán trực tiếp hộp/tứ giác bao quanh text.
%     \item \textbf{Connected-component based:} phát hiện thành phần ký tự và liên kết thành cụm.
%     \item \textbf{Segmentation-based:} dự đoán bản đồ pixel thuộc vùng text và tách instance bằng hậu xử lý.
% \end{itemize}
% Trong thực tế signboards, hướng segmentation-based phổ biến vì linh hoạt với chữ cong/biến dạng, nhưng gặp khó khi các cụm chữ nằm gần nhau gây chồng lấp.

% \subsection{Cơ sở lý thuyết Text Recognition}
% Text Recognition thường được mô hình hóa như bài toán nhận dạng chuỗi:
% \begin{itemize}
%     \item \textbf{CTC-based:} ánh xạ đặc trưng theo chiều ngang thành chuỗi ký tự với CTC loss.
%     \item \textbf{Attention/Encoder-Decoder:} sinh chuỗi theo cơ chế chú ý.
%     \item \textbf{Transformer-based recognizer:} tận dụng self-attention để học phụ thuộc dài và chống méo tốt hơn.
% \end{itemize}
% Với tiếng Việt, các yếu tố dấu và biến thể font làm tăng độ khó; hậu xử lý chuẩn hóa và từ điển miền (domain lexicon) có thể cải thiện độ chính xác.

% \subsection{Khai thác thông tin thời gian trong video}
% Khác ảnh tĩnh, video cho phép:
% \begin{itemize}
%     \item \textbf{Tracking text regions:} theo dõi một vùng chữ qua nhiều frame, giảm số lần chạy recognizer (nhận dạng một lần cho cả track).
%     \item \textbf{Temporal fusion:} hợp nhất nhiều kết quả nhận dạng theo vote/confidence/edit distance để tăng độ ổn định.
% \end{itemize}

% \section{Các nghiên cứu liên quan}

% \subsection{Bộ dữ liệu văn bản trong video lái xe: RoadText-1K}
% RoadText-1K giới thiệu bộ dữ liệu lớn cho bài toán phát hiện và nhận dạng văn bản trong video lái xe, gồm các đoạn clip được lấy mẫu từ dữ liệu lái xe thực tế và được gán nhãn dày (dense) theo từng frame.
% Bộ dữ liệu cung cấp:
% (i) bounding boxes cho vùng chữ, (ii) phiên âm (transcription), và (iii) nhãn phân loại text (ví dụ: tiếng Anh/không phải tiếng Anh/không đọc được), đồng thời tách riêng trường hợp biển số xe trong nhóm tiếng Anh.
% RoadText-1K được thiết kế theo hướng ``không thiên lệch theo text'' (unconstrained), phản ánh đúng bối cảnh camera hành trình với các nhiễu như motion blur, out-of-focus và glare.
% Các đánh giá baseline cho thấy các phương pháp SOTA trên ảnh tĩnh khi áp dụng vào video lái xe sẽ gặp suy giảm đáng kể do độ khó tăng lên.

% \subsection{Phương pháp phát hiện text theo cơ chế ``spotlight'': Spotlight Text Detector (STD)}
% Spotlight Text Detector (STD) tập trung giải quyết hai vấn đề lớn của text detection dạng segmentation:
% (i) các instance chữ nằm gần nhau gây chồng lấp khó tách, và (ii) hình dạng/độ dài chữ biến thiên lớn khiến mô hình khó khái quát.
% STD đề xuất hai thành phần chính:
% \begin{itemize}
%     \item \textbf{Spotlight Calibration Module (SCM):} hiệu chỉnh vùng ứng viên (candidate kernel) dựa trên coarse mask, tương tự cơ chế camera ``focus'' vào mục tiêu; module này giúp giảm false positives bằng cách hiệu chỉnh dự đoán và tăng khả năng tập trung vào vùng kernel quan trọng.
%     \item \textbf{Multivariate Information Extraction Module (MIEM):} trích xuất thông tin hình học đa dạng theo nhiều ``shape schemes'', nhằm học tốt hơn các đặc trưng tỷ lệ, hướng và hình dạng của chữ trong cảnh.
% \end{itemize}
% Kết quả thực nghiệm cho thấy STD đạt hiệu năng cạnh tranh/vượt trội trên nhiều benchmark text detection phổ biến (ICDAR2015, CTW1500, MSRA-TD500, Total-Text), đồng thời ablation chứng minh đóng góp của SCM và MIEM.

% \subsection{Liên hệ với đề tài khóa luận}
% Từ các nghiên cứu trên, có thể rút ra các định hướng quan trọng cho bài toán biển hiệu trong video street-view:
% \begin{itemize}
%     \item \textbf{Về dữ liệu và đánh giá:} cần ưu tiên bối cảnh ``unconstrained driving video'' và các dạng nhiễu đặc thù; RoadText-1K là nguồn tham khảo về cách thiết kế dữ liệu/nhãn và tiêu chí benchmark.
%     \item \textbf{Về phát hiện văn bản:} các phương pháp segmentation nâng cao cơ chế hiệu chỉnh/khoanh vùng (như SCM của STD) hữu ích khi text gần nhau và nền phức tạp --- đặc trưng thường gặp ở biển hiệu phố.
%     \item \textbf{Về pipeline tích hợp:} cần kết hợp (i) detection mạnh với chữ cong/biến dạng, (ii) rectify hợp lý trước recognition, và (iii) cơ chế temporal fusion/tracking để ổn định kết quả trên video.
% \end{itemize}

% \section{Tóm tắt chương}
% Chương này đã trình bày tổng quan bài toán đọc văn bản trên biển hiệu trong video đường phố, các thách thức đặc thù, cơ sở lý thuyết của hai thành phần chính (text detection và text recognition), cùng khả năng khai thác thông tin thời gian trong video.
% Ngoài ra, chương cũng tổng hợp hai hướng nghiên cứu liên quan tiêu biểu: bộ dữ liệu RoadText-1K cho video lái xe và phương pháp Spotlight Text Detector cho phát hiện chữ với cơ chế hiệu chỉnh vùng ứng viên.
% Những nội dung này là cơ sở để thiết kế pipeline tích hợp và xây dựng thực nghiệm trong các chương tiếp theo.
