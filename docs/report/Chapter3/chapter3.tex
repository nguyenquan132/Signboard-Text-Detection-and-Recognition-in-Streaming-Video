\chapter{PHƯƠNG PHÁP TIẾP CẬN}
\ifpdf
    \graphicspath{{Chapter3/Chapter3Figs/PNG/}{Chapter3/Chapter3Figs/PDF/}{Chapter3/Chapter3Figs/}}
\else
    \graphicspath{{Chapter3/Chapter3Figs/EPS/}{Chapter3/Chapter3Figs/}}
\fi


\section{Tổng quan về phương pháp}
Trong những năm gần đây, việc khai thác thông tin từ văn bản trên biển hiệu có ý nghĩa quan trọng trong việc xây dựng hệ sinh thái dữ liệu cho các ứng dụng dựa trên vị trí và quản lý đô thị. Xuất phát từ mục tiêu đó, khóa luận đề xuất một pipeline xử lý được thiết kế theo ba giai đoạn chính: (i) \textbf{Phát hiện biển hiệu}, (ii) \textbf{Phát hiện văn bản trong vùng biển hiệu}, và (iii) \textbf{Nhận dạng nội dung văn bản}. Theo đó, pipeline được minh họa trong Hình~\ref{fig:chapter3_pipeline_overview}, trong đó quá trình xử lý bắt đầu bằng việc phát hiện và trích xuất vùng biển hiệu từ ảnh đầu vào. Từ các vùng biển hiệu đã được cắt, hệ thống tiến hành phát hiện các vùng văn bản tương ứng, trước khi thực hiện nhận dạng nội dung văn bản từ các vùng đã được xác định. Dựa trên pipeline đề xuất, khóa luận tập trung phân tích và lựa chọn một số phương pháp tiên tiến hiện nay cho từng giai đoạn xử lý. Việc lựa chọn này được thực hiện dựa trên các nghiên cứu khảo sát gần đây trong lĩnh vực. Nội dung chi tiết cho từng giai đoạn sẽ được trình bày lần lượt trong các mục sau.

\begin{figure}[t]
    \centering
    % TODO: cập nhật đường dẫn theo project của bạn
    \includegraphics[width=1\linewidth]{assets/chapter3/pipeline_text_det_rec_on_signboard.png}
    \caption{Kiến trúc tổng quan của hệ thống phát hiện và nhận dạng văn bản trên biển hiệu}
    \label{fig:chapter3_pipeline_overview}
\end{figure}

\section{Phát hiện biển hiệu}
\subsection{Phát hiện đối tượng}
\label{sec:object_detection}
Trong giai đoạn phát hiện biển hiệu, khóa luận lựa chọn một số phương pháp tiêu biểu được đề cập trong các nghiên cứu gần đây \cite{zou2023object} để tiến hành đánh giá thực nghiệm.

\paragraph{YOLO (You Only Look Once)} YOLO \cite{redmon2016you} được đề xuất bởi Redmon và cộng sự, là đại diện tiêu biểu cho hướng tiếp cận một giai đoạn (one-stage) trong bài toán phát hiện đối tượng. Khác với các phương pháp hai giai đoạn, YOLO tiếp cận bài toán như một bài toán hồi quy toàn cục, trong đó mô hình dự đoán trực tiếp các hộp giới hạn (bounding boxes) cùng xác suất lớp (class probabilities) trên toàn bộ ảnh đầu vào. Kiến trúc tổng quát của YOLO được minh họa trong Hình~\ref{fig:chapter3_yolo_architecture}, bao gồm ba thành phần chính: backbone dùng để trích xuất đặc trưng, neck nhằm kết hợp đặc trưng đa tỉ lệ, và detection head để thực hiện dự đoán hộp bao và nhãn phân loại.

\begin{figure}[t]
    \centering
    % TODO: cập nhật đường dẫn theo project của bạn
    \includegraphics[width=1\linewidth]{assets/chapter3/yolo_architecture.png}
    \caption{Kiến trúc tổng quan của YOLO \cite{redmon2016you}}
    \label{fig:chapter3_yolo_architecture}
\end{figure}


Trải qua nhiều phiên bản phát triển, YOLO liên tục được cải tiến nhằm nâng cao độ chính xác trong khi vẫn duy trì hiệu quả tính toán. Do đó, khóa luận lựa chọn một số phiên bản YOLO gần đây, chẳng hạn như YOLOv8 và YOLOv11 để đưa vào thực nghiệm, qua đó đánh giá hiệu quả của phương pháp trong giai đoạn phát hiện biển hiệu. Bên cạnh đó, các biến thể YOLO hỗ trợ phát hiện hộp xoay (Oriented Bounding Box -- OBB) cũng được đưa vào đánh giá, nhằm xử lý hiệu quả hơn các trường hợp biển hiệu có hướng nghiêng hoặc hình dạng không song song với trục ảnh.

\paragraph{DETR (DEtection TRansformer)} DETR \cite{carion2020end} được đề xuất bởi Carion và cộng sự, là mô hình phát hiện đối tượng đầu tiên hoàn toàn dựa trên kiến trúc Transformer. Khác với các phương pháp dựa trên anchor truyền thống, DETR tiếp cận bài toán theo hướng dự đoán tập hợp (set prediction), trong đó mỗi đối tượng được ánh xạ trực tiếp thành một phần tử trong tập đầu ra thông qua cơ chế tự chú ý (self-attention). Cách tiếp cận này cho phép mô hình khai thác quan hệ ngữ cảnh trên toàn bộ ảnh và loại bỏ các bước hậu xử lý phức tạp như Non-Maximum Suppression (NMS). Hình~\ref{fig:chapter3_detr_architecture} minh họa kiến trúc DETR, trong đó sử dụng CNN backbone để trích xuất đặc trưng, sau đó Transformer encoder và decoder phối hợp mô hình hóa ngữ cảnh toàn cục và sinh ra tập hợp các dự đoán cuối cùng thông qua các object queries học được.

\begin{figure}[t]
    \centering
    % TODO: cập nhật đường dẫn theo project của bạn
    \includegraphics[width=1\linewidth]{assets/chapter3/detr_architecture.jpeg}
    \caption{Kiến trúc tổng quan của DETR \cite{carion2020end}}
    \label{fig:chapter3_detr_architecture}
\end{figure}

Trong bối cảnh phát hiện biển hiệu, DETR được lựa chọn như một phương pháp đại diện cho hướng tiếp cận dựa trên Transformer nhằm đánh giá khả năng khai thác ngữ cảnh toàn cục. Đặc biệt, cơ chế dự đoán tập hợp của DETR giúp giảm thiểu sự phụ thuộc vào các giả định hình học cục bộ, từ đó phù hợp với các trường hợp biển hiệu có bố cục đa dạng.

\paragraph{RT-DETRv2} Dựa trên ý tưởng tiếp cận end-to-end của DETR cho bài toán phát hiện đối tượng, Zhao và cộng sự giới thiệu RT-DETRv2 \cite{lv2024rt} như một phiên bản cải tiến của RT-DETR \cite{zhao2024detrs}, với mục tiêu tối ưu hóa hiệu suất thời gian thực trong khi vẫn duy trì độ chính xác cao. Mô hình này giữ nguyên ưu điểm loại bỏ các bước hậu xử lý như Non-Maximum Suppression (NMS), đồng thời được tăng cường bằng các cơ chế tối ưu nhằm cân bằng hiệu quả giữa tốc độ suy luận và chất lượng dự đoán. Kiến trúc của RTDETRv2, minh họa trong Hình~\ref{fig:chapter3_rtdetr_architecture}, dựa trên thiết kế RT-DETR gốc, nổi bật với: (i) hybrid encoder kết hợp ưu điểm của CNN trong trích xuất đặc trưng hiệu quả và Transformer trong mô hình hóa ngữ cảnh toàn cục, (ii) cơ chế lựa chọn truy vấn thích ứng giúp giảm số lượng truy vấn không cần thiết, qua đó cải thiện tốc độ suy luận mà ít ảnh hưởng đến độ chính xác.

\begin{figure}[t]
    \centering
    % TODO: cập nhật đường dẫn theo project của bạn
    \includegraphics[width=1\linewidth]{assets/chapter3/rtdetr_architecture.png}
    \caption{Kiến trúc tổng quan của RT-DETR, được sử dụng trong RTDETRv2 \cite{zhao2024detrs}}
    \label{fig:chapter3_rtdetr_architecture}
\end{figure}

Trên cơ sở đó, RTDETRv2 được lựa chọn như một phương pháp đại diện cho hướng tiếp cận Transformer tối ưu hóa cho thời gian thực, đặc biệt phù hợp với các trường hợp yêu cầu tốc độ xử lý cao như phân tích video giao thông hoặc cảnh đường phố.

\subsection{Phân đoạn ngữ nghĩa}
Bên cạnh các phương pháp phát hiện trực tiếp dựa trên bounding box, nhằm mở rộng
góc nhìn đánh giá, khóa luận xem xét thêm một hướng tiếp cận gián tiếp thông qua
bài toán phân đoạn ngữ nghĩa (semantic segmentation). Theo hướng tiếp cận này,
đối tượng được phân đoạn ở mức điểm ảnh, từ đó suy ra các vùng bao hình học phục vụ
cho bài toán phát hiện.

Trong bối cảnh đó, theo nghiên cứu khảo sát gần đây \cite{thisanke2023semantic}, các kiến
trúc dựa trên Transformer đã trở thành một hướng tiếp cận quan trọng và được quan
tâm rộng rãi trong bài toán phân đoạn ảnh, đặc biệt là phân đoạn ngữ nghĩa. Nhờ khả
năng mô hình hóa ngữ cảnh toàn cục thông qua cơ chế tự chú ý (self-attention), các mô hình này
cho thấy hiệu quả nổi bật trong việc xử lý các trường hợp phức tạp với sự đa dạng lớn
về hình dạng và bố cục của đối tượng.

\paragraph{SegFormer} SegFormer \cite{xie2021segformer}, được giới thiệu bởi Xie và cộng sự, là một kiến trúc phân đoạn ngữ nghĩa hiệu quả, kết hợp encoder Transformer phân cấp (hierarchical) và decoder MLP nhẹ, được minh họa trong Hình \ref{fig:chapter3_segformer_architecture}. Thiết kế này cho phép mô hình khai thác ngữ cảnh toàn cục ở nhiều tỷ lệ, đồng thời duy trì hiệu suất tính toán cao nhờ decoder đơn giản. Chính sự cân bằng giữa độ chính xác và tốc độ này khiến SegFormer trở thành một lựa chọn phù hợp để đánh giá hiệu quả của phân đoạn ngữ nghĩa trong việc phát hiện các biển hiệu, đặc biệt đối với các biển hiệu xuất hiện ở nhiều góc nghiêng khác nhau.

\begin{figure}[t]
    \centering
    % TODO: cập nhật đường dẫn theo project của bạn
    \includegraphics[width=1\linewidth]{assets/chapter3/segformer_architecture.png}
    \caption{Kiến trúc tổng quan của SegFormer \cite{xie2021segformer}}
    \label{fig:chapter3_segformer_architecture}
\end{figure}

\paragraph{Mask2Former} Mask2Former \cite{cheng2022masked} được đề xuất bởi Cheng và cộng sự, đại diện cho một hướng tiếp cận phân đoạn thống nhất (unified framework) dựa trên Transformer, có khả năng xử lý linh hoạt nhiều bài toán phân đoạn khác nhau như phân đoạn ngữ nghĩa (semantic segmentation), phân đoạn theo thể hiện (instance segmentation) và phân đoạn toàn cảnh (panoptic segmentation). Kiến trúc của Mask2Former được trình bày chi tiết trong Hình~\ref{fig:chapter3_mask2former_architecture}, áp dụng cơ chế chú ý có mặt nạ (masked attention), trong đó mỗi truy vấn tập trung vào các vùng đặc trưng liên quan đến mặt nạ (mask) dự đoán, thay vì toàn bộ không gian ảnh. Cách tiếp cận này giúp mô hình cải thiện khả năng biểu diễn hình dạng và ranh giới chi tiết của các đối tượng, đặc biệt hiệu quả trong các trường hợp đối tượng chồng lấn hoặc có cấu trúc hình học phức tạp. Nhờ đó, Mask2Former được lựa chọn để đánh giá khả năng xử lý các biển hiệu trong những trường hợp bị chồng lấn.
\begin{figure}[t]
    \centering
    % TODO: cập nhật đường dẫn theo project của bạn
    \includegraphics[width=1\linewidth]{assets/chapter3/mask2former_architecture.jpg}
    \caption{Kiến trúc tổng quan của Mask2Former \cite{cheng2022masked}}
    \label{fig:chapter3_mask2former_architecture}
\end{figure}

\section{Phát hiện và nhận dạng văn bản trên biển hiệu theo hướng tiếp cận hai giai đoạn (Two-Stage)}
\subsection{Phát hiện văn bản trên biển hiệu}
\label{sec:text_detection}
Trên cơ sở các vùng biển hiệu đã được xác định, hệ thống tiếp tục với nhiệm vụ phát hiện văn bản trên biển hiệu, được xem xét dưới góc độ của bài toán Scene Text Detection (STD). Để đánh giá hiệu quả, các phương pháp tiên tiến hiện nay được lựa chọn dựa trên các nghiên cứu khảo sát gần đây \cite{kang2022overview, pal2024comprehensive, naiemi2022scene}. 

\paragraph{PANet} PANet \cite{liu2018panet}, được đề xuất bởi Liu và cộng sự, là một kiến trúc phát hiện văn bản hiệu quả dựa trên nguyên tắc phân đoạn. Mô hình bao gồm hai thành phần chính: Feature Pyramid Enhancement Module (FPEM) để tạo bản đồ đặc trưng đa tỷ lệ, và Feature Fusion Module (FFM) để tổng hợp các đặc trưng này. Kiến trúc của PANet được minh họa trong Hình~\ref{fig:chapter3_panet_architecture}. Nhờ khả năng tập hợp các pixel văn bản thành các thể hiện tương ứng trên bản đồ đặc trưng cuối cùng, PANet có thể phát hiện văn bản chính xác mà vẫn duy trì hiệu suất tính toán cao, phù hợp với bài toán phát hiện văn bản trên biển hiệu với nhiều kích thước và hướng khác nhau.

\begin{figure}[t]
    \centering
    % TODO: cập nhật đường dẫn theo project của bạn
    \includegraphics[width=1\linewidth]{assets/chapter3/panet_architecture.png}
    \caption{Kiến trúc tổng quan của PANet \cite{liu2018panet}}
    \label{fig:chapter3_panet_architecture}
\end{figure}

\paragraph{DBNet++} DBNet++ \cite{liao2022real}, được đề xuất bởi Liao và cộng sự, là phiên bản cải tiến của DBNet \cite{liao2020real}, được thiết kế để phát hiện văn bản trong ảnh ngoại cảnh với độ chính xác cao và ổn định. Mô hình tích hợp cơ chế differentiable binarization (DB) trực tiếp vào mạng phân đoạn, giúp tạo mặt nạ (mask) văn bản chính xác và giảm đáng kể các bước hậu xử lý. Bên cạnh đó, module Adaptive Scale Fusion (ASF) được áp dụng để hợp nhất các đặc trưng đa tỷ lệ, nâng cao khả năng phát hiện các văn bản có kích thước khác nhau. Kiến trúc của DBNet++ được minh họa trong Hình~\ref{fig:chapter3_dbnet_plus_plus_architecture}. DBNet++ được lựa chọn nhờ khả năng xử lý hiệu quả các đường biên văn bản không rõ nét, đồng thời phát hiện chính xác cả các dòng chữ lớn (tiêu đề) và nhỏ (thông tin chi tiết) thường cùng xuất hiện trên một biển hiệu.

\begin{figure}[t]
    \centering
    % TODO: cập nhật đường dẫn theo project của bạn
    \includegraphics[width=1\linewidth]{assets/chapter3/dbnet_plus_plus_architecture.png}
    \caption{Kiến trúc tổng quan của DBNet++ \cite{liao2022real}}
    \label{fig:chapter3_dbnet_plus_plus_architecture}
\end{figure}

\paragraph{TextPMs} TextPMs \cite{zhang2022arbitrary} được đề xuất bởi Zhang và cộng sự, thay vì tạo trực tiếp mặt nạ (mask) nhị phân, TextPMs dự đoán một nhóm bản đồ xác suất (probability maps) bằng cách ánh xạ khoảng cách từ pixel đến đường biên đánh dấu (annotation boundary) thành giá trị xác suất, sử dụng các hàm Sigmoid Alpha (SAF). Sau khi dự đoán nhóm bản đồ xác suất, một mô hình học lặp (iterative model) được áp dụng để tổng hợp các bản đồ này, cung cấp thông tin đầy đủ cho việc tái tạo các thể hiện văn bản. Cuối cùng, thuật toán phát triển vùng (region growth) được sử dụng để gộp các bản đồ xác suất thành các đối tượng văn bản hoàn chỉnh. Quy trình dự đoán bản đồ xác suất và phát triển vùng của TextPMs được minh họa trong Hình~\ref{fig:chapter3_textpms_architecture}

Việc lựa chọn TextPMs dựa trên khả năng phát hiện hiệu quả các văn bản với hình dạng bất thường (như cong hoặc nghiêng), kích thước khác nhau và hướng đa dạng, đồng thời xử lý tốt các đường biên không rõ nét trên biển hiệu.

\begin{figure}[t]
    \centering
    % TODO: cập nhật đường dẫn theo project của bạn
    \includegraphics[width=1\linewidth]{assets/chapter3/textpms_architecture.png}
    \caption{Kiến trúc tổng quan của TextPMs \cite{liao2022real}}
    \label{fig:chapter3_textpms_architecture}
\end{figure}

\paragraph{FAST} Nhằm phát hiện các văn bản hình dạng tùy ý, đồng thời đảm bảo cả độ chính xác và tốc độ cao, Zhang và cộng sự đã giới thiệu FAST \cite{chen2021fast}, tập trung vào việc đơn giản hóa mô hình và tối ưu hóa quá trình xử lý. Thay vì dựa vào các kiến trúc phức tạp và hậu xử lý nặng, FAST đề xuất một biểu diễn kernel tối giản (minimalist kernel) với đầu ra 1 kênh để mô hình hóa văn bản có hình dạng tùy ý, kết hợp với một quá trình hậu xử lý song song trên GPU nhằm ghép nhanh các dòng chữ với chi phí thời gian không đáng kể. Đồng thời, kiến trúc mạng của FAST được tối ưu hóa thông qua tìm kiếm kiến trúc mạng (neural architecture search) chuyên cho bài toán phát hiện văn bản, giúp trích xuất các đặc trưng mạnh mẽ và phù hợp hơn so với các mạng được thiết kế cho phân loại ảnh. Hình~\ref{fig:chapter3_fast_architecture} cung cấp minh họa trực quan về kiến trúc tối ưu của FAST. Việc lựa chọn FAST dựa trên khả năng phát hiện hiệu quả các văn bản có hình dạng tùy ý, tối ưu cả về tốc độ lẫn độ chính xác, phù hợp với các biển hiệu xuất hiện ở nhiều kích thước, hình dạng và hướng khác nhau.

\begin{figure}[t]
    \centering
    % TODO: cập nhật đường dẫn theo project của bạn
    \includegraphics[width=1\linewidth]{assets/chapter3/fast_architecture.png}
    \caption{Kiến trúc tổng quan của FAST \cite{chen2021fast}}
    \label{fig:chapter3_fast_architecture}
\end{figure}

\paragraph{KPN} Để giải quyết vấn đề tách các thể hiện văn bản liền kề trong hình ảnh ngoại cảnh, một thách thức thường gặp với các văn bản có hình dạng tùy ý. Zhang và cộng sự đề xuất KPN \cite{zhang2022kernel}, sử dụng Kernel Proposal Network để dự đoán các bản đồ trung tâm Gaussian cho từng văn bản, từ đó trích xuất một tập hợp các kernel proposal động (dynamic convolution kernel) từ bản đồ đặc trưng embedding. Bên cạnh đó, để đảm bảo sự độc lập giữa các kernel, KPN áp dụng hàm mất mát học trực giao (orthogonal learning loss), kết hợp thông tin vị trí và thông tin ngữ nghĩa được mã hóa trong kernel. Các kernel này sau đó được áp dụng riêng rẽ lên bản đồ embedding nhằm tạo ra 
các bản đồ nhúng tương ứng với từng thể hiện văn bản, qua đó hỗ trợ phân tách 
rõ ràng các văn bản liền kề. Kiến trúc của KPN được minh họa trong Hình~\ref{fig:chapter3_kpn_architecture}. Với các đặc điểm trên, KPN được lựa chọn nhờ khả năng phân tách chính xác các văn bản liền kề, đặc biệt phù hợp với các biển hiệu chứa nhiều dòng chữ gần nhau hoặc ký tự dày đặc.

\begin{figure}[t]
    \centering
    % TODO: cập nhật đường dẫn theo project của bạn
    \includegraphics[width=1\linewidth]{assets/chapter3/kpn_architecture.png}
    \caption{Kiến trúc tổng quan của KPN \cite{zhang2022kernel}}
    \label{fig:chapter3_kpn_architecture}
\end{figure}


\paragraph{YOLO (OBB)} Bên cạnh các phương pháp tiên tiến cho phát hiện văn bản (Scene Text Detection - STD) đã được trình bày, khóa luận tiếp tục mở rộng đánh giá bằng cách áp dụng một số mô hình phát hiện đối tượng được giới thiệu ở Mục~\ref{sec:object_detection} (Phát hiện biển hiệu), cụ thể là phiên bản YOLOv8-OBB và YOLOv11-OBB. Do được huấn luyện ban đầu trên dữ liệu đối tượng tổng quát (general object), các mô hình này cần được fine-tune trên tập dữ liệu văn bản chuyên biệt. Việc đánh giá này nhằm xác định tính khả thi và hiệu quả của các kiến trúc phát hiện đối tượng khi chuyển giao (transfer) sang bài toán phát hiện văn bản, đặc biệt trong việc xử lý các dòng chữ nghiêng và có kích thước nhỏ trên biển hiệu.

\subsection{Nhận dạng nội dung văn bản}
\label{subsec:sota_rec}
Sau khi xác định các vùng chứa văn bản trên biển hiệu, hệ thống tiếp tục với giai đoạn nhận dạng nội dung văn bản. Giai đoạn này được tiếp cận như một bài toán Nhận dạng văn bản trong ảnh ngoại cảnh (Scene Text Recognition - STR), với mục tiêu chuyển đổi các vùng văn bản đã được phát hiện thành chuỗi ký tự tương ứng. Trên cùng cơ sở tiếp cận như giai đoạn phát hiện văn bản, khóa luận lựa chọn một số phương pháp STR tiên tiến hiện nay để tiến hành thực nghiệm và đánh giá, dựa trên các phân loại và hướng tiếp cận được tổng hợp từ các nghiên cứu khảo sát gần đây \cite{kang2022overview, pal2024comprehensive, naiemi2022scene}.

\paragraph{ViTSTR} ViTSTR \cite{atienza2021vision}, được đề xuất bởi Atienza và cộng sự, 
đại diện cho một hướng tiếp cận đơn giản và hiệu quả khi áp dụng kiến trúc Vision Transformer 
cho bài toán nhận dạng văn bản trong ảnh ngoại cảnh. Mô hình sử dụng kiến trúc một giai đoạn, 
bao gồm 12 khối encoder Transformer giống nhau và không sử dụng decoder, như được minh họa 
trong Hình~\ref{fig:chapter3_vitstr_architecture}. Trong thiết kế này, việc dự đoán được thực hiện 
thông qua một lớp tuyến tính, ánh xạ trực tiếp các đặc trưng đã được mã hóa thành chuỗi ký tự đầu ra. Nhờ kiến trúc tối giản, ViTSTR đạt được hiệu quả tính toán cao, thể hiện qua tốc độ suy luận nhanh và số lượng tham số nhỏ, tạo ra một giải pháp nhẹ và nhanh phù hợp với giai đoạn nhận dạng văn bản trên biển hiệu. Ngoài ra, mô hình còn áp dụng các kỹ thuật tăng cường dữ liệu đa dạng nhằm cải thiện độ chính xác.

\begin{figure}[t]
    \centering
    % TODO: cập nhật đường dẫn theo project của bạn
    \includegraphics[width=1\linewidth]{assets/chapter3/vitstr_architecture.jpg}
    \caption{Kiến trúc tổng quan của ViTSTR \cite{atienza2021vision}}
    \label{fig:chapter3_vitstr_architecture}
\end{figure}

\paragraph{PARSeq} Nhằm khắc phục những hạn chế của các mô hình ngôn ngữ tự hồi quy (autoregressive - AR) truyền thống, Bautista và cộng sự đã giới thiệu PARSeq \cite{bautista2022scene}. Phương pháp này tận dụng kỹ thuật Permutation Language Modeling để huấn luyện một tập hợp các mô hình ngôn ngữ AR nội bộ có trọng số chung, qua đó cho phép kết hợp linh hoạt giữa suy luận không tự hồi quy mang tính độc lập ngữ cảnh (context-free non-AR) và suy luận tự hồi quy có xét đến ngữ cảnh chuỗi (context-aware AR). Trên cơ sở đó, PARSeq tích hợp cơ chế tinh chỉnh lặp (iterative refinement) dựa trên ngữ cảnh hai chiều nhằm khai thác hiệu quả thông tin ngữ cảnh mà không cần đến mô hình ngôn ngữ bên ngoài hay quy trình xử lý nhiều giai đoạn phức tạp. Nhờ vậy, mô hình thể hiện tính mạnh mẽ trước các văn bản có hướng và bố cục đa dạng, giúp nâng cao hiệu quả cho giai đoạn nhận dạng văn bản trên biển hiệu. Kiến trúc tổng quan của PARSeq được trình bày trong Hình \ref{fig:chapter3_parseq_architecture}.

\begin{figure}[t]
    \centering
    % TODO: cập nhật đường dẫn theo project của bạn
    \includegraphics[width=1\linewidth]{assets/chapter3/parseq_architecture.png}
    \caption{Kiến trúc tổng quan của PARSeq \cite{bautista2022scene}}
    \label{fig:chapter3_parseq_architecture}
\end{figure}

\paragraph{CDistNet} Để khắc phục hạn chế trong việc kết hợp thông tin thị giác và ngữ nghĩa vốn thường không được căn chỉnh chính xác, đặc biệt đối với các mẫu văn bản có bố cục phức tạp hoặc biến dạng mạnh, Zheng và cộng sự đã đề xuất CDistNet \cite{zheng2024cdistnet}. Phương pháp này nhằm tăng cường mối liên kết chặt chẽ giữa hai miền đặc trưng, qua đó cải thiện khả năng căn chỉnh giữa đặc trưng và ký tự trong quá trình nhận dạng. CDistNet sử dụng một encoder gồm ba nhánh song song để trích xuất các nguồn thông tin bổ sung cho nhau, bao gồm đặc trưng thị giác từ ảnh đầu vào, đặc trưng ngữ nghĩa từ chuỗi ký tự, và embedding vị trí mô tả quan hệ không gian giữa các ký tự. Các đặc trưng này sau đó được đưa vào mô-đun Multi-Domain Character Distance Perception (MDCDP) tạo ra một embedding vị trí (positional embedding) để đồng thời truy vấn cả đặc trưng thi giác và ngữ nghĩa thông qua cơ chế chú ý chéo (cross-attention). Thông qua cơ chế này, CDistNet có khả năng trực tiếp mô hình hóa khoảng cách ký tự đa miền, bao gồm khoảng cách không gian, mối quan hệ ngữ nghĩa giữa các ký tự, cũng như sự liên kết giữa hai loại thông tin này. Cấu trúc encoder ba nhánh và mô-đun MDCDP của CDistNet được minh họa trong Hình~\ref{fig:chapter3_cdistnet_architecture}.

\begin{figure}[t]
    \centering
    % TODO: cập nhật đường dẫn theo project của bạn
    \includegraphics[width=1\linewidth]{assets/chapter3/cdistnet_architecture.png}
    \caption{Kiến trúc tổng quan của CDistNet \cite{zheng2024cdistnet}}
    \label{fig:chapter3_cdistnet_architecture}
\end{figure}

Bằng cách xếp chồng nhiều mô-đun MDCDP, mô hình dần dần học được sự căn chỉnh chính xác hơn giữa vùng ảnh và ký tự, ngay cả trong các trường hợp nhận dạng khó. Nhờ đó, CDistNet hiệu quả giai đoạn nhận dạng văn bản trên biển hiệu với kiểu chữ biến dạng, xoay nghiêng hoặc bố cục không chuẩn.

\paragraph{SMTR} SMTR \cite{du2025out}, được đề xuất bởi Du và cộng sự, áp dụng hướng tiếp cận dựa trên so khớp chuỗi con (sub-string matching) nhằm khắc phục hạn chế của các phương pháp truyền
thống trong việc nhận dạng các chuỗi văn bản dài. Thay vì dự đoán toàn bộ chuỗi cùng lúc,
SMTR thực hiện nhận dạng thông qua một quy trình lặp. Cụ thể, mô hình sử dụng hai mô-đun dựa trên cơ chế chú ý chéo (cross-attention), trong đó mô-đun thứ nhất mã hóa một chuỗi con gồm nhiều ký tự thành các truy vấn ngữ cảnh trước và sau, trong khi mô-đun thứ hai khai thác các truy vấn này để chú ý vào đặc trưng hình ảnh, đồng thời nhận dạng ký tự kế tiếp và ký tự liền trước của chuỗi con. Quá trình này được lặp lại nhiều lần, cho phép SMTR nhận dạng văn bản có độ dài tùy ý. Dựa trên cơ chế nhận dạng chuỗi con, SMTR có thể được huấn luyện trên các tập dữ liệu văn bản ngắn nhưng vẫn tổng quát tốt cho văn bản dài. Sơ đồ mô-đun và quy trình lặp của SMTR được trình bày trong Hình \ref{fig:chapter3_smtr_architecture}.

Ngoài ra, SMTR tích hợp chiến lược tăng cường suy luận (inference augmentation strategy)
nhằm giảm thiểu sự nhầm lẫn giữa các chuỗi con tương tự. Nhờ đó, mô hình cải thiện đáng kể
hiệu quả nhận dạng các chuỗi văn bản dài và phức tạp, đặc biệt phù hợp với các biển hiệu
chứa nhiều từ hoặc các dòng chữ kéo dài.

\begin{figure}[t]
    \centering
    % TODO: cập nhật đường dẫn theo project của bạn
    \includegraphics[width=1\linewidth]{assets/chapter3/smtr_architecture.png}
    \caption{Kiến trúc tổng quan của SMTR \cite{du2025out}}
    \label{fig:chapter3_smtr_architecture}
\end{figure}

\paragraph{SVTRv2} Du và cộng sự đã giới thiệu SVTRv2 \cite{du2025svtrv2}, là phiên bản mở rộng của SVTR \cite{du2022svtr}, được đề xuất nhằm khắc phục những hạn chế về độ chính xác của các mô hình dựa trên Connectionist Temporal Classification (CTC) khi xử lý văn bản có hình dạng bất thường hoặc thiếu ngữ cảnh ngôn ngữ (linguistic missing), mặc dù các mô hình này vốn có ưu điểm về kiến trúc đơn giản và tốc độ suy luận nhanh so với các mô hình encoder-decoder. SVTRv2 áp dụng chiến lược đa kích thước (multi-size resizing) để điều chỉnh kích thước ảnh đầu vào phù hợp, tránh biến dạng nghiêm trọng, đồng thời giới thiệu mô-đun sắp xếp lại đặc trưng (feature rearrangement) để đảm bảo đặc trưng thị giác phù hợp với yêu cầu căn chỉnh của CTC. Bên cạnh đó, SVTRv2 tích hợp mô-đun định hướng ngữ nghĩa (semantic guidance module) nhằm đưa thông tin ngôn ngữ vào quá trình học đặc trưng thị giác, giúp mô hình tận dụng ngữ cảnh chuỗi để cải thiện độ chính xác. Đáng chú ý, mô-đun này chỉ được sử dụng trong giai đoạn huấn luyện và có thể loại bỏ hoàn toàn khi suy luận, do đó không làm tăng chi phí tính toán khi triển khai thực tế. Hình \ref{fig:chapter3_svtrv2_architecture} minh họa kiến trúc tổng quan của SVTRv2.

\begin{figure}[t]
    \centering
    % TODO: cập nhật đường dẫn theo project của bạn
    \includegraphics[width=1\linewidth]{assets/chapter3/svtrv2_architecture.png}
    \caption{Kiến trúc tổng quan của SVTRv2 \cite{du2025svtrv2}}
    \label{fig:chapter3_svtrv2_architecture}
\end{figure}

Dựa trên sự kết hợp giữa hiệu quả suy luận của CTC và khả năng mô hình hóa các văn bản có hình dạng bất thường, cũng như khai thác ngữ cảnh ngôn ngữ, SVTRv2 đạt được sự cân bằng tốt giữa tốc độ và độ chính xác trong các trường hợp nhận dạng văn bản đa dạng như văn bản dài và văn bản có hình dạng phức tạp. Do đó, trong giai đoạn nhận dạng văn bản trên biển hiệu, SVTRv2 cho thấy tính phù hợp cao
nhờ khả năng cân bằng giữa tốc độ suy luận và độ chính xác.

\section{Phát hiện và nhận dạng văn bản trên biển hiệu theo hướng tiếp cận một giai đoạn (One-Stage)}
\label{sec:one_stage_methods}
Mặc dù hướng tiếp cận hai giai đoạn (two-stage), trong đó phát hiện và nhận dạng văn bản được thực hiện riêng biệt, đã cho thấy hiệu quả và tính linh hoạt cao trong bài toán nhận dạng văn bản trên biển hiệu, các phương pháp một giai đoạn (one-stage) ngày càng thu hút sự quan tâm nhờ khả năng suy luận trực tiếp từ ảnh đầu vào đến chuỗi ký tự đầu ra trong một mô hình thống nhất. Do đó, bên cạnh việc xây dựng và đánh giá pipeline hai giai đoạn, khóa luận tiến hành thực nghiệm so sánh giữa hai chiến lược tiếp cận: (i) hướng tiếp cận hai giai đoạn (two-stage), với hai mô-đun riêng biệt cho phát hiện và nhận dạng; và (ii) hướng tiếp cận một giai đoạn (one-stage), sử dụng các mô hình tiên tiến như TESTR, DeepSolo, UNITS, và DNTextSpotter.

Mục tiêu của so sánh nhằm phân tích ưu nhược điểm của từng chiến lược trong bối cảnh nhận dạng văn bản trên biển hiệu, đặc biệt xét trên các khía cạnh như độ chính xác, tốc độ suy luận, mức độ phức tạp của hệ thống, đồng thời định hướng lựa chọn mô hình phù hợp để tinh chỉnh (fine-tuning) hiệu quả và tiết kiệm thời gian huấn luyện. Qua đó, khóa luận cung cấp cái nhìn tổng quan về khả năng ứng dụng thực tế của các phương pháp phát hiện và nhận dạng văn bản thống nhất (Text Spotting) hiện đại.

\paragraph{TESTR} TESTR \cite{zhang2022text} được đề xuất bởi Zhang và cộng sự, nổi bật với việc áp dụng kiến trúc Transformer cho việc phát hiện và nhận dạng văn bản đầu-cuối (end-to-end) trong ảnh ngoại cảnh. Mô hình xây dựng dựa trên một bộ mã hóa (encoder) chung và hai bộ giải mã (decoder) song song, lần lượt đảm nhiệm việc hồi quy các điểm điều khiển của hộp chữ (text-box control point regression) và nhận dạng ký tự. Thiết kế này giúp TESTR loại bỏ hoàn toàn các thao tác trích xuất vùng quan tâm (RoI) và các quy trình hậu xử lý phức tạp dựa trên heuristic. Trên cơ sở đó, TESTR đặc biệt hiệu quả khi xử lý các văn bản uốn cong và có hình dạng bất kỳ nhờ biểu diễn linh hoạt bằng đường cong Bezier hoặc đa giác, thay vì chỉ sử dụng hộp giới hạn hình chữ nhật truyền thống. Bên cạnh đó, quy trình phát hiện đa giác có định hướng từ hộp giới hạn (box-to-polygon detection) được đề xuất nhằm nâng cao độ chính xác định vị. Kiến trúc tổng quan của TESTR được minh họa trong Hình \ref{fig:chapter3_testr_architecture}.

\begin{figure}[t]
    \centering
    % TODO: cập nhật đường dẫn theo project của bạn
    \includegraphics[width=1\linewidth]{assets/chapter3/testr_architecture.png}
    \caption{Sơ đồ kiến trúc của TESTR \cite{zhang2022text}}
    \label{fig:chapter3_testr_architecture}
\end{figure}

\paragraph{DeepSolo} DeepSolo \cite{ye2023deepsolo}, được giới thiệu bởi Ye và cộng sự, nhằm giải quyết bài toán phát hiện và nhận dạng văn bản thống nhất (end-to-end) trong ảnh ngoại cảnh, nổi bật với khả năng xử lý đồng thời cả hai nhiệm vụ trong một mô hình duy nhất. Dựa trên nền tảng kiến trúc DETR, DeepSolo sử dụng một bộ giải mã (decoder) duy nhất với cơ chế Explicit Points Solo, cho phép mô hình học đồng thời để phát hiện và nhận dạng văn bản. Mỗi thực thể văn bản được biểu diễn dưới dạng chuỗi các điểm sắp xếp thứ tự, và được mô hình hóa thông qua các truy vấn điểm có thể học được (learnable explicit point queries). Sau khi đi qua decoder, các truy vấn này mã hóa thông tin ngữ nghĩa và vị trí của văn bản, từ đó có thể giải mã để xác định đường trung tâm (center line), biên giới (boundary), kiểu chữ (script) và độ tin cậy (confidence) thông qua các đầu ra dự đoán song song đơn giản. Nhờ những đặc điểm này, DeepSolo đạt hiệu quả cao cả về độ chính xác lẫn tốc độ huấn luyện trên các bộ dữ liệu chuẩn, đồng thời cung cấp giải pháp linh hoạt, phù hợp cho bài toán nhận dạng văn bản trên biển hiệu. Sơ đồ khối (block diagram) của DeepSolo được thể hiện trong Hình \ref{fig:chapter3_deepsolo_architecture}.

\begin{figure}[t]
    \centering
    % TODO: cập nhật đường dẫn theo project của bạn
    \includegraphics[width=1\linewidth]{assets/chapter3/deepsolo_architecture.png}
    \caption{Kiến trúc tổng quan của DeepSolo \cite{ye2023deepsolo}}
    \label{fig:chapter3_deepsolo_architecture}
\end{figure}

\paragraph{UNITS} Nhằm khắc phục một số hạn chế về định dạng phát hiện và số lượng văn bản trong các mô hình tự hồi quy (auto-regressive) trước đó, Kil và cộng sự đã đề xuất UNITS (UNIfied Text Spotter) \cite{kil2023towards}, nổi bật với khả năng thống nhất nhiều định dạng phát hiện, bao gồm tứ giác (quadrilateral) và đa giác (polygon), giúp mô hình xử lý văn bản có hình dạng bất kỳ. UNITS hoạt động theo cơ chế tạo chuỗi (sequence generation), trong đó thông tin của mỗi thể hiện văn bản (text instance) trong chuỗi đầu ra bao gồm token định dạng phát hiện, các token tọa độ cho việc định vị và chuỗi ký tự nhận dạng. Đặc biệt, kỹ thuật starting-point prompting được tích hợp, cho phép mô hình bắt đầu trích xuất văn bản từ một vị trí bất kỳ, từ đó có thể phát hiện nhiều thực thể văn bản vượt quá số lượng mà mô hình đã được huấn luyện. Pipeline hoạt động theo cơ chế tạo chuỗi (sequence generation) của UNITS được trình bày trong Hình ~\ref{fig:chapter3_units_architecture}. Nhờ khả năng mở rộng linh hoạt này, UNITS được lựa chọn là một giải pháp phù hợp cho giai đoạn phát hiện và nhận dạng văn bản trên biển hiệu, đặc biệt trong các trường hợp văn bản có hình dạng đa dạng và mật độ cao.

\begin{figure}[t]
    \centering
    % TODO: cập nhật đường dẫn theo project của bạn
    \includegraphics[width=1\linewidth]{assets/chapter3/units_architecture.png}
    \caption{Pipeline tổng quan của UNITS \cite{kil2023towards}}
    \label{fig:chapter3_units_architecture}
\end{figure}

\paragraph{DNTextSpotter} DNTextSpotter \cite{qiao2024dntextspotter}, được giới thiệu bởi Qiao và cộng sự, nhằm cải thiện độ ổn định và hiệu quả huấn luyện trong các phương pháp phát hiện và nhận dạng văn bản đầu-cuối (end-to-end text spotting) dựa trên kiến trúc Transformer. Các đặc trưng đa tỉ lệ (multi-scale features) được trích xuất từ backbone và bộ mã hóa (encoder), sau đó được đưa vào một bộ giải mã (decoder) với thiết kế hai nhánh đặc biệt: (i) gồm phần ghép cặp (matching part), sử dụng các truy vấn khởi tạo ngẫu nhiên và tính toán hàm mất mát thông qua thuật toán ghép cặp đồ thị hai phía (bipartite graph matching), và (ii) phần khử nhiễu (denoising part) căn chỉnh giữa vị trí và nội dung bằng các truy vấn vị trí nhiễu (noised positional queries) và truy vấn nội dung nhiễu (noised content queries). Trong đó, các truy vấn vị trí được tạo ra từ bốn điểm điều khiển Bezier của đường trung tâm, còn các truy vấn nội dung được khởi tạo thông qua phương pháp trượt ký tự có mặt nạ (masked character sliding), đồng thời một hàm mất mát bổ sung cho việc phân loại ký tự nền được tích hợp nhằm tăng cường khả năng nhận biết ngữ cảnh. Hình \ref{fig:chapter3_dntext_architecture} minh họa kiến trúc DNTextSpotter. 

Dưa trên cơ chế khử nhiễu và khả năng căn chỉnh vị trí-nội dung hiệu quả, D\allowbreak N\allowbreak T\allowbreak e\allowbreak x\allowbreak t\allowbreak S\allowbreak p\allowbreak o\allowbreak t\allowbreak t\allowbreak e\allowbreak r chọn cho bài toán phát hiện và nhận dạng văn bản trên biển hiệu với hình dạng đa dạng và phức tạp.

\begin{figure}[t]
    \centering
    % TODO: cập nhật đường dẫn theo project của bạn
    \includegraphics[width=1\linewidth]{assets/chapter3/dntext_architecture.png}
    \caption{Kiến trúc tổng quan của DNTextSpotter \cite{qiao2024dntextspotter}}
    \label{fig:chapter3_dntext_architecture}
\end{figure}